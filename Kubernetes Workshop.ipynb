{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# → https://bit.ly/fat-k8s-workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Workshop Loading QR Code](bit.ly_fat-k8s-workshop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* setting up a generic Kubernetes system\n",
    "* deploying Laravel onto it using Helm charts (versionable infrastructure description), with Postgres, Redis and workers\n",
    "* employing a Gitlab repository for CI/CD for building container images\n",
    "* managing per-deployment configuration and environment variables\n",
    "* process health, logging and scaling\n",
    "* using the artisan CLI tool for scheduled and one-off jobs on the cluster (as time permits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export GIT_COMMITTER_NAME=$JUPYTERHUB_USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/kubernetes-workshop\n",
    "./setup_account.sh\n",
    "chmod go-r ~/.kube/config\n",
    "export USER=$GIT_COMMITTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubernetes Workshop\n",
    "\n",
    "With Phil Weir\n",
    "(the guy who is somewhere near this projector, hopefully)\n",
    "\n",
    "* Structure of course\n",
    "* Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic notebook instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Shift+Enter or Ctrl+Enter to execute a \"cell\" (each one of these rows)\n",
    "* Those with [ ] in the left gutter are executable - the others are informational\n",
    "* Up/Down arrows keys are the easiest way to navigate the notebook\n",
    "* If you select a cell (click to the left, outside the text area), the 'b' key will give you a new executable box below\n",
    "* If you accidentally edit an informational cell - it will switch to markdown - Ctrl+Enter will exit edit-mode\n",
    "* If a step hangs, with a [ * ] on the left, you may need to click the recycle button in the toolbar (if you have any other issues afterwards, re-run the cell at the top, to set environment variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Kubernetes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is Kuernetes\n",
    "    * Back in the day, if you wanted to run programs on a bunch of servers you had to log in and out of them all manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is this relevant to Python?\n",
    "  * Scalable\n",
    "  * Systematic\n",
    "  * Within control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why Kubernetes over alternatives?\n",
    "  * Other container orchestrators\n",
    "    * Popularity and backing\n",
    "    * Good balance of simplicity and complexity\n",
    "  * IaaS/PaaS\n",
    "    * Doesn't tie you to a single provider\n",
    "    * Can be run off-cloud\n",
    "    * Can be run on a dev machine\n",
    "    * Abstracts and automates the provider-specific bits (mostly)\n",
    "    * Separates out hardware from application infrastructure (containers vs servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Brief (re)introduction to Docker\n",
    "  * If you aren't already familiar with it, you can think of a docker _container_ as a VM that shares a kernel with the host\n",
    "    * ...but don't as that loses key important differences\n",
    "  * Recommended practice is to have one process per container\n",
    "    * containers are very memory light compared to VMs, so this is much less wasteful than it may sound\n",
    "    * this provides encapsulation and makes scaling easier\n",
    "  * `docker-compose` is an _extremely_ handy tool that takes a short, app-specific _docker-compose.yml_ file as input and spins up a multi-container environment with all the expected dependencies and links\n",
    "    * you can keep the `docker-compose.yml` file in the repo with your app\n",
    "    * between docker and docker-compose, you can provide a rough alternative to VirtualBox and Vagrant developer flow (although those do not map directly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cloud\n",
    "  * GKE: gcloud\n",
    "  * AWS: EKS\n",
    "  * Azure: AKS\n",
    "  * IBM: IKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manually (kubeadm / Kubernetes the Hard Way --->)\n",
    "  * kops\n",
    "  * kubeadm\n",
    "  * [Kubernetes the Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)\n",
    "  * For more information, see https://kubernetes.io/docs/setup/scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Minikube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, I will talk a little bit about basic Kubernetes tools and concepts, then we can start building up practical steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language of Kubernetes communication is JSON, under the hood, but generally the tooling lets you talk to it in YAML, which is actually a (much more readable) superset. Instead of\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"apiVersion\": \"v1\",\n",
    "    \"type\": \"Pod\",\n",
    "    \"metadata\": {\n",
    "        \"name\": \"hi-pod\"\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "we can write\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "type: Pod\n",
    "metadata:\n",
    "    name: \"hi-pod\"\n",
    "```\n",
    "\n",
    "We can build up all our Kubernetes objects on the cluster by sending declarative YAML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Just a webserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to interact with a Kubernetes cluster is the `kubectl` tool. It's preinstalled here, but you can download it yourself locally. The configuration files are called `kubeconfig` files, and the default one lives at `~/.kube/config` on Mac/*nix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*nginx* is an extremely popular webserver. We can run it on Kubernetes with no further information -- this is the equivalent of logging into a Linux server and setting up Apache (or, indeed, nginx), but in one handy line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create deployment mynginx --image=nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _deployment_ is essentially a single application (perhaps running many times). It is normally one or more replicas of a specific process (in this case nginx), maybe with some helper process or some start-up/shutdown actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run two identical instances of nginx, by scaling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl scale deployment mynginx --replicas=2\n",
    "kubectl get deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for us to interact with nginx we have to encounter another concept: _services_ . Commonly, we often think of a process and the service it provides as essentially equivalent - however, modern \"orchestration\" tools help decouple the idea of an advertised service (_services_) and the processes backing them up (_deployments_).\n",
    "\n",
    "This means that we can link up different tools with them only knowing each others' *services* - Kubernetes will route requests through to the deployed processes behind the scenes. If we have multiple processes running, it applies simple load balancing (\"Round Robin\" by default, passing one task/request to each processes in turn as they come in). For example, there could be 100 nginx processes spread over a dozen VMs and any internal or external request just asks for the nginx service and magically gets a reply.\n",
    "\n",
    "We created a deployment, but we still need to create that service... as with `create` for deployments, there is a command to attach a service: `expose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl expose deployment mynginx --port=80\n",
    "kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain service types that involve Kubernetes creating and attaching an external load balancer on the cloud platform - so, for instance, you can tell an AWS-based Kubernetes cluster that you want an external load balancer to point to nginx, and it will tell AWS to fire one up and show the `EXTERNAL IP` in this list, without you having to plumb it in. Your nginx processes will then be available publicly (by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short workshop to cover a lot of concepts, but lets take a brief break to check that works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP=$(kubectl get service mynginx --output=jsonpath=\"{.spec.clusterIP}\")\n",
    "curl http://$IP | displayHTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a step hangs, click the stop (⏹) button in the toolbar to restart bash, and re-run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple and consistent, we will use mostly private IPs - so we can't navigate to the public URL in a browser, and instead show the output in a notebook - but the jump to public on a cloud provider is also pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final bit of machinery we should see before we move on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pods_ are the individual processes running on a machine somewhere. Each of those is an nginx process. They are the basic unit of Kubernetes execution - here, two Pods were created as part of our Deployment (remember, we scaled it to 2 processes)\n",
    "\n",
    "Strictly, pods _can_ be more than one process. They are usually, but not always, one Docker container (which is _generally_ one OS process) - however, in some cases a \"sidecar\" container is useful. For instance, a metric-exporter process running alongside your Python app, might send regular resource usage (CPU/memory/IO) statistics to a database so you can graph performance over time. Once you see the `1/1` appearing under `READY` above, this means that all 1 of the 1 containers in the Pod are good to go.\n",
    "\n",
    "In any case, conceptually, a Pod still represents one instance of one tool/process. They are created and destroyed as part of Deployments, Jobs, and many other Kubernetes objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we tidy up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete deployment mynginx\n",
    "kubectl delete service mynginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All gone! (you might need to try a couple of times, while you wait for it to terminate) As the Deployment was deleted, the pods that made it up, went away too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are only a few of the Kubernetes concepts and objects, and the above is an intentionally hand-wavey introduction, to give you a feel for some key components. For the moment, we'll steer back to PHP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* docker-compose\n",
    "  * like orchestration-lite\n",
    "  * ties in closely with docker-swarm, but equally useful standalone\n",
    "  * allows your process and connections to be described in one file\n",
    "  * helps you manage variables, linking and persistance\n",
    "  * great test-bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Minikube\n",
    "  * a virtual machine for using Kubernetes on your own laptop\n",
    "  * avoids running cloud servers for development/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Helm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting up Helm\n",
    "  * Helm describes itself as a Kubernetes package manager\n",
    "  * What this means is that a whole set of interlinking components, secrets, services can be described under one banner and jointly deployed as a \"Chart\"\n",
    "  * This could be a full app, or a self-backing-up database, or Thingsboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helm can tell us what charts are installed already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helm list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output is blank - this is because we haven't installed any charts (i.e. packages) so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without getting too much into all the `helm` functionality, let's start by seeing how we can deploy an nginx server to our Kubernetes cluster. Helm uses Kubernetes libraries under the hood, so your default authentication configuration (from ~/.kube/config) will get used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helm repo update\n",
    "helm search hub python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We register a Helm repo (repository) to pull these charts (apps) from - for many more, see https://artifacthub.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helm repo add python-fastapi-postgres https://archish27.github.io/python-fastapi-postgres-helm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can install from that repo - in this case, a trivially simply Python API that responds `{\"Hello\": \"World\"}` to requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helm install my-python-fastapi-postgres python-fastapi-postgres/python-fastapi-postgres --version 0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the below that a whole series of objects have been created, including a Deployment and a Service. As we learned above, a Deployment will create one or more Pods - you can see the Pod starts with the same name and is marked \"related\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the above may need re-run a few times until it stops showing \"Init:0/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP=$(kubectl get service my-python-fastapi-postgres --output=jsonpath=\"{.spec.clusterIP}\")\n",
    "curl http://$IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how this type of tool is built up from Python and Dockerfiles soon, but for now, the key takeaway is that you can take a Python app, dockerize it, and run it in a process on Kubernetes (here, a deployment, consisting of one pod, consisting of one container, which runs one Python process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helm also gives us a neat way of cleaning up all the different related pieces for this app in one go -- deployments, services, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helm delete my-python-fastapi-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and sure enough (perhaps after a few seconds)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Concepts of charts and Docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension**: So what actually _is_ a Chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a filetree of, mostly, template [YAML](https://en.wikipedia.org/wiki/YAML) files. Each of these defines a Kubernetes object, such as a Deployment, Service or Secret. This way you can have all the boilerplate in a single git-versioned Chart, dynamically dropping in per-deployment settings using a handful of templated variables at deployment time - such the public URL for nginx, or the back-up retention period for postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm install wordpress bitnami/wordpress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(re-run the above until an external IP address appears and the pods say `1/1` - may take a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "echo \"Internal: \" $(kubectl get service wordpress --output=jsonpath=\"{.spec.clusterIP}\")\n",
    "echo \"External: http://$(kubectl get service wordpress --output=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\"\n",
    "echo \"Administration: http://$(kubectl get service wordpress --output=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")/wp-admin\"\n",
    "  echo Username: user\n",
    "  echo Password: $(kubectl get secret --namespace administrator wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 -d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helm delete wordpress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we visualize this? F&T often works with PHP applications - these have a webserver (nginx), which sends data to a PHP process (phpfpm), which gets data from a database (mariadb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PHP diagram](https://s3.us-west-2.amazonaws.com/public.flaxandteal.co.uk/larakube-1-things.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a similar idea in Python..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Example App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ~/kubernetes-workshop-flask-example; cd ~\n",
    "git clone https://github.com/flaxandteal/kubernetes-workshop-flask-example ~/kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/kubernetes-workshop-flask-example\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a few tools for Kubernetes, in particular an example file for setting deployment-specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat values.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try installing this Helm chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/kubernetes-workshop-flask-example\n",
    "helm install python-course-021cc ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in every type of deployment, we need to have specific custom settings for our own deployment. This answers several questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach:\n",
    "\n",
    "* **Where is our app code stored?**: In the final built images. During continuous integration (or manual building, for smaller scale), pre-prepared base images have the code added in. The built images are then tagged for that code version and pushed to a Docker image registry that the Kubernetes cluster can access.\n",
    "* **How do we keep our code secure?**: In this example, for simplicity we use the default base images, without custom app code - as such, they are pulled down from the public Docker Hub registry. However, there are private image registries within each cloud platform, e.g. AWS ECR, which Kubernetes can authenticate with to pull down images.\n",
    "* **What about secrets?**: In this particular approach, we use Kubernetes Secrets, which is simple but only a minimum bar (although, the default implementation is improving). Better practice is to use something like the Kubernetes Vault Operator, but this takes more care to set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_None of these are hard and fast_. There are various approaches, with benefits and drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the above until it quietens down and they all say \"Running\" or \"Completed\"^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP=$(kubectl get service python-course-021cc-kubernetes-workshop-flask-example --output=jsonpath=\"{.spec.clusterIP}\")\n",
    "curl http://$IP:5000/alembic_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above returns `null` this is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Work through the charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at what just started up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have only one service, it contains our code: https://github.com/flaxandteal/kubernetes-workshop-flask-example-app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send our output to stdout by default (i.e. simply `print()` and `logging`), we find that the logs from this Pod are the logs of a `Flask`/`gunicorn` webserver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASK_POD=$(kubectl get pods -o=name  --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example)\n",
    "kubectl logs $FLASK_POD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach also simplified life for log aggregation, where logs can be gathered from the stdout/stderr of the various Pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is CI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github provides free basic continuous integration (CI) - when done well, this is an easy, reproducible and consistent way to go from local development to built images. To follow along, you will need to have [your own Github account or sign up](https://github.com), or can look through the [details of the pipeline](https://github.com/flaxandteal/kubernetes-workshop-flask-example-app/actions/) of the existing demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to use your own account, then you [should fork](https://github.com/flaxandteal/kubernetes-workshop-flask-example-app/fork) a copy. You can clone it down here, or on your own machine/VM to make it easier to experiment. If you want to use your fork in this notebook, then set your Github username here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export GITHUB_USERNAME=philtweir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example app is a microservice for doing Alchemy with SQL (the result of a bad pun that I couldn't let go) - we can create the Philosopher's Stone (Magnum Opus) by requiring several Substances (Mercury, Salt and Sulphur) through web requests, mixing them to get Gloop, and then cooking, washing, pickling and fermenting them in that order, to obtain the Philosopher's Stone of legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(for anyone wondering, while vastly over-simplified, this does reflect the basic structure of the alchemical technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the [app itself](https://github.com/philtweir/kubernetes-workshop-flask-example-app), we have a repository for the infrastructure code (the Helm chart) used to run it: https://github.com/philtweir/kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get down to our lowest level for today. The Python code has to be wrapped up into a `container` - this is what it sounds like, a ringfenced set of files and resources that cannot see the outside world, except as explicitly permitted. They are similar, but different to, light virtual machines (side-note: but all share one operating system kernel when running on a laptop/server) - they may be as small as tens of megabytes, or very large, but they have their own filesystem. Two key objectives are that they can be reproducibly built - through a Dockerfile, which states the recipe - and are isolated enough to run anywhere without needing to know much about their environment (we will mention the \"12 factor app\" later that codifies this philosophy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we can take a look at the `Dockerfile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd ~\n",
    "git clone https://github.com/philtweir/kubernetes-workshop-flask-example-app\n",
    "cd ~/kubernetes-workshop-flask-example-app; git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts from a well-known base - an official Python Docker image, with a very cut-down operating system (Alpine), and builds on top. Alpine is great for very light, small containers (which is important if you want to run 100s or 1000s in parallel on a few servers, to serve web requests in high volume) but has some optimisations that can make it difficult to use with the whole Python ecosystem -- there are also Debian and Ubuntu based images, that have the same commands and behaviour to your Linux servers/laptops but might be a little bulkier (e.g. `python:3.8-slim`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next commands build on top. As we are starting from a nearly empty operating system, we start by adding a user, and then copy our files from the repo into the container. We tell the container that it will be running as a user called `user` and will expose a port `5000` where the Python webserver (gunicorn) will expect to see web requests. Have a look at `gunicorn_config.py` for a bit more info. Finally, we copy the app itself in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build this locally with `docker build .` but the next question is, how does the container image (the output of this build) get moved onto Kubernetes to run as a new app? In short, we use Continuous Integration - handy short-lived servers that see changes comining into a git repository and re-run any commands like docker build, pushing container images to a *container (image) registry* that Kubernetes can see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pro-tip: Modern Kubernetes has Role Based Authorization Control (RBAC), so we can create deployment users with special privileges that can ensure changes go directly from Github to our Kubernetes cluster without our intervention. Historically, it was common use `kubectl` to run these commands, but now tools like AgroCD or FluxCD automate the process more robustly. It would be possible do this via `curl` calls also, to the Kubernetes API. A particular benefit of Github Actions and, for example, Gitlab-CI, is that they can run for free on the code-hoster's platform, but can also be run internally, or even _on_ Kubernetes, if you need more speed or resources when building Docker images.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not the only options, however - Bitbucket now has a similar set-up. Jenkins, CircleCI, Concourse and others can provide these types of pipeline also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: the CI steps show some of the Docker commands that could be run locally, if you wanted to get the hang of Docker in a manual way, building images and pushing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CI on a fork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbal instructions will walk you through forking the repository and creating your own version (and CI). Once that is working, try the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_USERNAME=philtweir\n",
    "kubectl set image deployment/python-course-021cc-kubernetes-workshop-flask-example kubernetes-workshop-flask-example=ghcr.io/${GITHUB_USERNAME}/kubernetes-workshop-flask-example-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that it is running, what have we got? First, let us see the services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed above with Wordpress, we can use IP addresses, but within the cluster, it manages its own DNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Internal: \"http://$(kubectl get service python-course-021cc-kubernetes-workshop-flask-example --output=jsonpath=\"{.spec.clusterIP}\")/alembic_instruction\n",
    "echo \"Equivalent alternative: http://python-course-021cc-kubernetes-workshop-flask-example.${JUPYTERHUB_USER}:5000/alembic_instruction\"\n",
    "curl http://python-course-021cc-kubernetes-workshop-flask-example.${JUPYTERHUB_USER}:5000/alembic_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is `SERVICENAME.NAMESPACE` or, equivalently, `SERVICENAME.NAMESPACE.cluster.svc.local`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The namespace is a grouping mechanism, like multitenancy, where each of you can have your own resources, like services and pods, that do not interfere with each other. For convenience, I have chosen to give everybody a namespace name that is simply their username."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an app, we still need to do something with it. Remember I said it's a fancy kitchen mixer for the Philosopher's Stone? Let us do a little bash scripting to make it easier to poke at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get () {\n",
    "    curl python-course-021cc-kubernetes-workshop-flask-example.$JUPYTERHUB_USER:5000$1\n",
    "}\n",
    "post () {\n",
    "    curl    -H 'Content-Type: application/json' -X POST python-course-021cc-kubernetes-workshop-flask-example.$JUPYTERHUB_USER:5000$1 --data \"$2\"\n",
    "}\n",
    "delete () {\n",
    "    curl    -H 'Content-Type: application/json' -X DELETE python-course-021cc-kubernetes-workshop-flask-example.$JUPYTERHUB_USER:5000$1 --data \"$2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is nothing to do with Kubernetes - in fact, it is bash scripting that lets us call curl without typing the same parameters every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "echo Empty the Pantry \\(forget any Substances that were already stored\\)\n",
    "delete /substance\n",
    "\n",
    "echo Create some Substances, so we can mix them\n",
    "post /substance '{\"nature\": \"Mercury\"}'\n",
    "post /substance '{\"nature\": \"Salt\"}'\n",
    "post /substance '{\"nature\": \"Sulphur\"}'\n",
    "\n",
    "echo Mix the Substances to get Gloop in our Alembic \\(old-school mixing pot\\)\n",
    "post /alembic_instruction '{\"instruction_type\": \"mix\", \"natures\": \"Mercury,Salt,Sulphur\"}'\n",
    "\n",
    "echo Play with the Gloop until we have the Philosopher\\'s Stone\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"cook\"}'\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"wash\"}'\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"pickle\"}'\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"ferment\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, that does not look good. How do we find out where all the errors came from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes debugging, as a system that has many levels, is a bigger topic than an introduction. However, to help you see where to start - try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLASK_POD=$(kubectl get pods -o=name  --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example)\n",
    "kubectl logs $FLASK_POD | tail -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of logs. However, we should recognise that we are looking at Python exceptions - that is our first clue. Our second is that it tells us a database table is missing... if we have a database (which our app does), then we should initialize it! To do this, we can use the handy `kubectl exec` to execute a pre-defined initialization routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl exec -ti deploy/python-course-021cc-kubernetes-workshop-flask-example -- python -m magnumopus.initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "echo Empty the Pantry \\(forget any Substances that were already stored\\)\n",
    "delete /substance\n",
    "\n",
    "echo Create some Substances, so we can mix them\n",
    "post /substance '{\"nature\": \"Mercury\"}'\n",
    "post /substance '{\"nature\": \"Salt\"}'\n",
    "post /substance '{\"nature\": \"Sulphur\"}'\n",
    "\n",
    "echo Mix the Substances to get Gloop in our Alembic \\(old-school mixing pot\\)\n",
    "post /alembic_instruction '{\"instruction_type\": \"mix\", \"natures\": \"Mercury,Salt,Sulphur\"}'\n",
    "\n",
    "echo Play with the Gloop until we have the Philosopher\\'s Stone\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"cook\"}'\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"wash\"}'\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"pickle\"}'\n",
    "post /alembic_instruction '{\"instruction_type\": \"process\", \"natures\": \"Gloop\", \"action\": \"ferment\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging doesn't just stop here though, we can also examine more configuration-type issues, outside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl set image deployment/python-course-021cc-kubernetes-workshop-flask-example kubernetes-workshop-flask-example=nonexistant/image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that, having set it to run from a non-existant Docker image, the Python server no longer exists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes is quite clever - as you have tried to update a deployment, but the resulting pod has not been successful, Kubernetes keeps the old one running until the problem is resolved, so your running service does not break (if it can be avoided).\n",
    "\n",
    "To find out more, we can use the `kubectl describe` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe pods --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the events section (near the end), you will see a whole set of recurring errors and their frequencies. These are being sent by the `kubelet` daemon, which manages the running of actual containers on VMs (nodes) - it has noticed that it cannot pull the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can happen quite a bit if you have, for example, private container registry credentials that are expiring too fast, are missing or are incorrect. If you want to find out more about private registry credentials, have a look at the `imagePullSecrets` setting for Pods/Deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can restore the image, like so (or with a `helm upgrade ...`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl set image deployment/python-course-021cc-kubernetes-workshop-flask-example kubernetes-workshop-flask-example=ghcr.io/$GITHUB_USERNAME/kubernetes-workshop-flask-example-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes provides some useful tools for managing roll-out. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl rollout history deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give a bit more info by adding a certain annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl annotate deployment/python-course-021cc-kubernetes-workshop-flask-example kubernetes.io/change-cause='fix version'\n",
    "kubectl set image deployment/python-course-021cc-kubernetes-workshop-flask-example kubernetes-workshop-flask-example=ghcr.io/${GITHUB_USERNAME}/kubernetes-workshop-flask-example-app\n",
    "kubectl rollout history deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes manages lots of its decisions by forms of \"tagging\", where `annotations` and/or `labels` are used to mark a deployment/service/pod for specific behaviour or treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get deployment python-course-021cc-kubernetes-workshop-flask-example -o yaml | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels, for instance, can be used as a handy filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get deployments --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get pods --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can roll back deployments, you can require a certain minimum number of live pods at any time during the rolling update, or even decide where new Pods are scheduled on the underlying VMs/systems (termed Nodes). Kubernetes has concepts of liveness and readiness probes, so for some types of deployment, it can spot when Pods are functional or not. In addition, this allows it to undertake one of the most fundamental aspects of orchestration, replacing Pods when they become unresponsive or die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens if we restart the pod? (because pods are \"stateless\", this is equivalent to deleting it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl delete pods --selector=app.kubernetes.io/name=kubernetes-workshop-flask-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post /substance '{\"nature\": \"Mercury\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks like our database disappeared. We have no concept of state right now, so nothing is saved when a pod/container restarts. We won't get into the details of `volumes` (except verbally) but suffice it to say, having an in-memory database lasts only as long as the memory. Let's try adding a real database. Helm makes this as easy as one command for most major (open source) databases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can take a [12 factor](https://12factor.net/) approach to override aspects of the app. Firstly, we can add a postgres server..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm upgrade --install db bitnami/postgresql #  --set primary.resources.requests.cpu=100m --set primary.resources.requests.memory=100Mi --set primary.resources.limits.cpu=100m --set primary.resources.limits.memory=100Mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a password (in production systems there is much more to secret management)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "POSTGRES_PASSWORD=$(kubectl get secret db-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 -d)\n",
    "echo $POSTGRES_PASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we upgrade our chart to apply some new values - we override an environment variable (one that the Python app knows about) to give Postgres credentials, and this time, rather than passing the image as a separate setting with `set image` or `--set`, we put it all together in a single `values` file. Think of this like per-environment configuration that gets rolled into your Chart or app when it is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd ~/kubernetes-workshop-flask-example\n",
    "git pull\n",
    "echo '\n",
    "env:\n",
    "    DATABASE_URI: \"postgresql://postgres:'${POSTGRES_PASSWORD}'@db-postgresql:5432/postgres\"\n",
    "image:\n",
    "    repository: \"ghcr.io/'${GITHUB_USERNAME}'/kubernetes-workshop-flask-example-app\"\n",
    "' > local.yaml\n",
    "helm upgrade python-course-021cc . --values local.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post /alembic_instruction '{\"instruction_type\": \"mix\", \"natures\": \"Mercury,Salt,Sulphur\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a new DB, so we need to run our initialization again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kubectl exec -ti deploy/python-course-021cc-kubernetes-workshop-flask-example -- python -m magnumopus.initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but let's do it a slightly neater way - rather than hacking in to an existing pod, Kubernetes lets us create a `job` which is a single-execution task, that gets its own container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl create job python-course-initialize-db --image=ghcr.io/${GITHUB_USERNAME}/kubernetes-workshop-flask-example-app -- /bin/sh -c \"export DATABASE_URI='postgresql://postgres:${POSTGRES_PASSWORD}@db-postgresql:5432/postgres'\n",
    "    python -m magnumopus.initialize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get jobs\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a pod successfully finishes, you can see it is `Completed`. Many charts provide post-installation jobs like this automatically, to avoid the need for manual steps, but sometimes it can cause confusion if actions are taken out of sequence, or before a database is ready, so don't assume too much when writing your Chart..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl delete job python-course-initialize-db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's poke at the DB a little. Some bash scripting again will help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_sql_on_pg () {\n",
    "  kubectl exec -ti db-postgresql-0 -- /bin/sh -c \"PGPASSWORD=$POSTGRES_PASSWORD psql -U postgres -c '$1'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_sql_on_pg \"\\dt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post /substance '{\"nature\": \"Mercury\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_sql_on_pg \"SELECT * from substances\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now does state stay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl delete pods --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post /substance '{\"nature\": \"Sulphur\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(it may take a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_sql_on_pg \"SELECT * from substances\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't spend time on volumes, but just to note that `persistent volumes` are the Kubernetes term for things like \"disks\" or cloud-based storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kubectl get pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to a (snapshotable) storage disk in GCP/AWS/Azure/IBM, and usually survives after the pods using it (in this case db-postgresql-0) die - the association is called being `Bound`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Try deleting the postgres helm installation with `helm delete`. Create it again. Did the data survive? (use run_sql_on_pg to check)\n",
    "\n",
    "Now try deleting both the postgres helm installation and, after, delete the pvc (persistent volume claim). Now reinstall postgres with helm. Has the PV come back? Is it the same? Is the data still there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cronjobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cronjobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make scheduled tasks work, we use the Kubernetes concept of Cronjobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get cronjobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see this uses a similar format to traditional cron schedules. As with the normal Laravel approach, we set this to run an artisan job every minute (`php artisan schedule:run`), and Laravel's scheduler will decide whether any of the PHP-defined tasks are due to get kicked off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite configurable - we can say how many completed Pods we want to keep, for diagnostics, and how many failed ones, how long we give them to start up, and how many attempts each will get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Running artisan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part of a broader concept of Jobs. These are requests to schedule a one-off Pod to do some task or other. Kubernetes' Cronjobs are really just creating a \"Job\" each minute from a template. Jobs in general provide a reasonable way to run one-off artisan tasks also.\n",
    "\n",
    "At present, we use a barebones script to simplify this process - running it creates a fresh new Job (from the template as the Cronjob, as it happens), and sets the internal artisan arguments to whichever command line flags we pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/buckram/kubernetes\n",
    "./artisan.sh route:list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get a few Errors as it starts up (ending \"ContainerCreating\"), but it should follow the Pod's logs once the Pod has started. Note that this approach is non-interactive (in fact, asynchronous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you should see a range of Jobs that have been run, including your manual one as \"laravel-job\" and a timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any other setting, you will want to have more than one app platform running at once: usually for development, staging and production; or perhaps blue/green deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gitlab provide tools to help manage separate environments, along with Kubernetes - some of that is quite tied to Google Kubernetes Engine, but some is platform agnostic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a separate Helm `values.yaml` file may be sufficient to give, e.g. a dev and staging cluster. You may be happy enough to have these both on the same Kubernetes cluster, treating it as an infrastructure provider, as long as they are namespaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes does provide namespacing - the divisions are being hardened at each version, and it is possible to apply resource usage policies, user role policies and network segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -n kube-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that there are a number of Pods doing more fundamental things than nginx - providing internal DNS, managing Helm's requests, handling Kubernetes API calls, running controller loops. These are all in the `kube-system` namespace by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore everything in your own namespace - what does it all do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubectl works on a concept of users, contexts and clusters. You might not have spotted yet, but Kubernetes does not have a concept of user accounts, per se. It does have a concept of authentication, which can be by certificates, tokens, OpenID Connect, for example, and authorization Roles, which are bound to a username matching a RoleBinding rule when it successfully authenticates. However, no separate \"User\" object exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contexts allow you to specify a user, a namespace and a cluster to work with by default. Switching contexts allows you to easily manage multiple clusters and namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl config get-contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `~/buckram/docs` folder is a script, `update_cluster_to_ci_build_ref.sh`, to update one context from another - this can be very handy for manual promotion, where you want to have specific controls in place to control access to the update workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingresses allow you to add rules for redirecting traffic on a domain to a certain service. `kubectl` has a handy describe mode (with most objects) - for the Jupyterhub ingress, for example, it looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ kubectl get ingress --namespace jupyterhub jupyterhub-internal\n",
    "Name:             jupyterhub-internal\n",
    "Namespace:        jupyterhub\n",
    "Address:          35.239.24.66\n",
    "Default backend:  default-http-backend:80 (10.8.0.4:8080)\n",
    "TLS:\n",
    "  kubelego-tls-proxy-jupyterhub terminates scotphp.flaxandteal.co.uk\n",
    "Rules:\n",
    "  Host                       Path  Backends\n",
    "  ----                       ----  --------\n",
    "  scotphp.flaxandteal.co.uk  \n",
    "                             /   proxy-http:8000 (<none>)\n",
    "Annotations:\n",
    "Events:\n",
    "  Type    Reason  Age   From                      Message\n",
    "  ----    ------  ----  ----                      -------\n",
    "  Normal  CREATE  57m   nginx-ingress-controller  Ingress jupyterhub/jupyterhub-internal\n",
    "  Normal  UPDATE  56m   nginx-ingress-controller  Ingress jupyterhub/jupyterhub-internal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding YAML looks something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: extensions/v1beta1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: jupyterhub-internal\n",
    "  namespace: jupyterhub\n",
    "spec:\n",
    "  rules:\n",
    "  - host: scotphp.flaxandteal.co.uk\n",
    "    http:\n",
    "      paths:\n",
    "      - backend:\n",
    "          serviceName: proxy-http\n",
    "          servicePort: 8000\n",
    "        path: /\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - scotphp.flaxandteal.co.uk\n",
    "    secretName: TLSSECRET\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, you will notice that there is a Kubernetes Secret required. This represents the SSL certificate. One option to manage these, which is an evolution of `kube-lego` amongst other tools is `cert-manager` - it can be installed as a Helm chart and will watch for Ingresses to work out which certificates to request (and from where). For more information, see [http://docs.cert-manager.io/en/latest/](http://docs.cert-manager.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note for minikube users: if using this with minikube, you will need to run `minikube addons enable ingress` on the host. When an ingress is created, you should also add a rule to `/etc/hosts` to direct traffic for that domain to the output of `minikube ip`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health, Logging and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of monitoring solutions available. CoreOS, one of the driver organizations behind containerization, have provided a [Kubernetes Operator for Prometheus](https://github.com/coreos/prometheus-operator) - Operators are a relative recent, powerful tool for making Kubernetes more extensible, and allowing automated control loops, for example, to take care of dynamically-defined types of object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing this operator, which can be done with `helm`, adds in several new types of Kubernetes object: PrometheusRule and AlertingRule being two examples. The operator can then keep a cluster-hosted Prometheus server dynamically changing its configuration as those are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional Helm chart, [kube-prometheus](https://github.com/coreos/prometheus-operator/tree/master/helm/kube-prometheus) builds on this to provide live feeds of Pod, Service, Node resource usage, and alerting based off that. We have implemented a basic monitoring chart that adds support for Laravel failed job and exception tracking and alerts. While some of this can be done through the framework, this ensures that metrics and alerting can be managed centrally, and that individual, scaled out web-serving or queue processes are not responsible for contacting external APIs - rather they feed it back internally to be grouped, rate-managed, etc. in a standard way with the rest of the infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Log aggregation (fluentbit, fluentd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the deployment, we create fluent-bit DaemonSets - these are essentially Deployments that run one Pod on each Node (VM). Gathering logs is a perfect application for them. Fluent-bit is a reimplementation in C of a lot of Fluentd's functionality. Fluent-bit can gather logs from the containers' stdout/stderr and send these to Fluentd, or to a number of other aggregators, such as Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helm list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl logs $(kubectl get pods --selector=app=buckram-fluentd -o=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autoscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to scale the number of deployed Pods automatically, using a HorizontalPodAutoscaler object, which responds to [predefined or custom metrics](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/). This is simplest to set up for CPU requests, but can work off, for example, [Prometheus metrics](https://github.com/directxman12/k8s-prometheus-adapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, [Cluster Autoscaling](https://github.com/kubernetes/autoscaler) (where the number of Nodes changes) is cloud provider dependent. This is well-established especially for GKE, but also available for other Kubernetes providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minikube\n",
    "\n",
    "(time permitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "minikube start\n",
    "[wait]\n",
    "kubectl get nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will show you one node - the VM that is running Kubernetes, and all that is on it. In normal usage, you would have a larger set of nodes - many setups will have a pool of nodes for masters, usually high in CPU resources, and a pool of nodes, perhaps smaller and more plentiful for granular horizontal scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the VM itself, once it has settled down, by running `minikube ssh`. Even more, you can see where Kubernetes rubber hits the road - if you run `docker ps`, you will see all the containers that make up Kubernetes and its apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an nginx deployment on minikube - check the output of `minikube ip` to see where it will be visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished, you can stop minikube with `minikube stop`, or delete it with `minikube delete`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker-Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have `docker-compose` installed, clone down the https://gitlab.com/flaxandteal/buckram-demo-with-sample-docker Gitlab repository locally (your fork if you have it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -v $(pwd):/app composer install # or any other means of running composer, if you have it locally\n",
    "cp .env.example .env\n",
    "chmod -R ugo+rwX storage/logs bootstrap/cache   \n",
    "./dartisan key:generate\n",
    "./dartisan migrate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see a blank Laravel app at `localhost:8000` in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run `make:auth` to scaffold login functionality. Can you get this running in Gitlab-CI and update it on the cluster here? In general, don't forget any migration and seeding that needs done on the live tool with `./artisan.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (includes some Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from a blank Laravel app. Run:\n",
    "```\n",
    "git init\n",
    "git submodule add https://gitlab.com/flaxandteal/buckram-starter infrastructure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Python script (if you are comfortable to do so) from ./infrastructure/python - this can be done with `pip3 install --user .` for instance and run `bookcloth local:initialize` in that directory. This should create a default docker-compose setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push this to a new public Gitlab repo, and check it runs the CI. Update the images as above to run it in this cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed this workshop and am looking forward to feedback!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Follow-up course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Shell Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Development:\n",
    "```\n",
    "export PATH=~/.local/bin:$PATH\n",
    "git clone https://gitlab.com/flaxandteal/buckram-demo-with-sample-docker\n",
    "cd buckram-demo                                                                                                                                  \n",
    "docker run -v $(pwd):/app composer install\n",
    "git submodule init\n",
    "git submodule update\n",
    "(cd infrastructure; git pull origin master; git checkout master)\n",
    "cp .env.example .env\n",
    "./dartisan key:generate\n",
    "./dartisan migrate\n",
    "lynx localhost:8000\n",
    "```\n",
    "\n",
    "Helm Setup:\n",
    "```\n",
    "wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz \n",
    "tar -xzf helm-v2.11.0-linux-amd64.tar.gz\n",
    "mv linux-amd64/helm ~/.local/bin\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
